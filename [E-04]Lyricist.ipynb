{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192a308e",
   "metadata": {},
   "source": [
    "# [E-04]Lyricist ( 작사가 )\n",
    "_____\n",
    "# 목차\n",
    "## 1. 개요\n",
    "    1.1 들어가기에 앞서\n",
    "    1.2 루브릭 평가기준\n",
    "## 2. 프로젝트: 멋진 작사가 만들기\n",
    "    2.1 데이터 읽어오기\n",
    "    2.2 데이터 정제\n",
    "    2.3 데이터셋 분리\n",
    "    2.4 모델 학습 및 평가\n",
    "## 3. 결론\n",
    "    3.1 참조\n",
    "    3.2 결론\n",
    "   \n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0001c16",
   "metadata": {},
   "source": [
    "## 1. 개요\n",
    "### 1.1 들어가기에 앞서\n",
    "\n",
    "#### 이론\n",
    "\n",
    "##### GPT-2\n",
    "- 작문할 수 있는 딥러닝 모델을 말한다.\n",
    "\n",
    "##### 언어모델\n",
    "- n−1개의 단어 시퀀스 w{1}, ... , w{n-1}가 주어졌을 때, n번째 단어 w{n}으로 무엇이 올지 예측하는 확률 모델\n",
    "\n",
    "##### 토큰화(Tokenize)\n",
    "- 문장을 일정한 기준으로 쪼개는 과정\n",
    "\n",
    "##### 소스 문장(Source Sentence)\n",
    "- 자연어처리 분야에서 모델의 입력이 되는 문장(x_train)\n",
    "\n",
    "##### 타겟 문장(Target Sentence)\n",
    "- 정답 역할을 하게 될 모델의 출력 문장(y_train)\n",
    "\n",
    "##### 벡터화(vectorize)\n",
    "- tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 한 번에 해준다\n",
    "\n",
    "##### 텐서(tensor)\n",
    "- 숫자로 변환된 데이터\n",
    "\n",
    "##### Embedding\n",
    "- 단어를 추상적으로 변환하는 역할\n",
    "\n",
    "##### RNN1(LSTM),RNN2(LSTM) \n",
    "- 문장을 순차적으로 읽으며 단어 간의 연관성을 분석\n",
    "\n",
    "##### Linear(Dense)\n",
    "- RNN에서 만들어낸 결과를 바탕으로 생성할 단어를 결정\n",
    "\n",
    "----\n",
    "\n",
    "### 1.2 루브릭 평가기준\n",
    "\n",
    "평가문항|상세기준\n",
    "-|-\n",
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?|텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?\n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?|특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?\n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?|텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89998c3",
   "metadata": {},
   "source": [
    "## 2. 프로젝트: 멋진 작사가 만들기\n",
    "### 2.1 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843e29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import 하기\n",
    "\n",
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824dd68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ad5b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c41b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b19eb",
   "metadata": {},
   "source": [
    "1. sentence = sentence.lower().strip()\n",
    "- 소문자로 바꾸고, 양쪽 공백을 지운다.\n",
    "\n",
    "2. sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "- 특수문자 양쪽에 공백을 넣는다.\n",
    "\n",
    "3. sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "- 여러개의 공백은 하나의 공백으로 바꾼다.\n",
    "\n",
    "4. sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "- a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다\n",
    "\n",
    "5. sentence = sentence.strip()\n",
    "- 양쪽 공백을 지운다.\n",
    "\n",
    "6. sentence = '<start> ' + sentence + ' <end>'\n",
    "- 문장 시작에는 <start>, 끝에는 <end>를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d649b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    word_list = preprocessed_sentence.split() \n",
    "    if len(word_list) > 15: continue # 토큰의 개수 15개 초과하면 제외\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과 10개 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31268ccd",
   "metadata": {},
   "source": [
    "토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 위해\n",
    "- if len(word_list) > 15: continue를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ba0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f249c5650a0>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer를 만들기\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없음\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞추기\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb0cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6263]\n",
      " [   2   15 2967  871    5    8   11 5739    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6386f373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea5c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497a8cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b2d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40267ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만들기\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c10df",
   "metadata": {},
   "source": [
    "- enc_train, dec_train 를 하나로 묶어 dataset으로 만든다.\n",
    "- enc_val, dec_val 를 하나로 묶어 val_dataset으로 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e01240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512 # embedding_size: 워드 벡터의 차원수\n",
    "hidden_size = 2048 # hidden_size: LSTM 레이어의 hidden state 의 차원수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fe6ff",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터인 embedding_size와 hidden_size를 조절하며 loss와 val_loss값 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1bdce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 8.11766367e-05, -1.47675528e-04,  1.69841675e-04, ...,\n",
       "          1.88826991e-04,  1.91001673e-05,  2.40910122e-05],\n",
       "        [ 2.99572595e-04, -7.05683415e-05,  3.05791269e-04, ...,\n",
       "          1.66958576e-04, -6.74303228e-05,  1.60884782e-04],\n",
       "        [ 2.09231934e-04, -4.38166375e-04,  5.16903063e-04, ...,\n",
       "          2.56792875e-04, -1.67655569e-04,  4.67032485e-04],\n",
       "        ...,\n",
       "        [-1.86625705e-03, -2.40157358e-03,  7.79911585e-04, ...,\n",
       "         -1.08572864e-03, -9.94169386e-04,  9.37105273e-04],\n",
       "        [-1.90591579e-03, -2.79953983e-03,  4.18997108e-04, ...,\n",
       "         -1.05133140e-03, -8.44379014e-04,  1.44546584e-03],\n",
       "        [-1.92077784e-03, -3.17416457e-03,  4.69305669e-05, ...,\n",
       "         -9.74238792e-04, -6.88639295e-04,  1.96792139e-03]],\n",
       "\n",
       "       [[ 8.11766367e-05, -1.47675528e-04,  1.69841675e-04, ...,\n",
       "          1.88826991e-04,  1.91001673e-05,  2.40910122e-05],\n",
       "        [ 2.29892001e-04, -1.74049695e-04,  3.83280014e-04, ...,\n",
       "          3.05402587e-04, -1.20038630e-05,  9.42980114e-05],\n",
       "        [ 2.04464071e-04, -7.23732519e-05,  6.69423549e-04, ...,\n",
       "          4.70249564e-04, -6.04669694e-05,  4.60181509e-05],\n",
       "        ...,\n",
       "        [ 5.45361196e-04, -1.16617687e-03,  1.55663444e-03, ...,\n",
       "          1.47130666e-03, -1.25438115e-03,  3.50363640e-04],\n",
       "        [ 5.22737158e-04, -1.29028608e-03,  1.45909819e-03, ...,\n",
       "          1.18087186e-03, -1.29304046e-03,  2.28552934e-04],\n",
       "        [ 2.80223991e-04, -1.46671908e-03,  1.17474922e-03, ...,\n",
       "          1.06644630e-03, -1.02915859e-03,  4.54220426e-04]],\n",
       "\n",
       "       [[ 8.11766367e-05, -1.47675528e-04,  1.69841675e-04, ...,\n",
       "          1.88826991e-04,  1.91001673e-05,  2.40910122e-05],\n",
       "        [ 1.61842050e-04, -5.19171066e-04,  2.53522769e-04, ...,\n",
       "          3.53484531e-04, -8.58423737e-05,  5.39417029e-04],\n",
       "        [ 3.25085392e-04, -5.15106425e-04,  3.56183969e-04, ...,\n",
       "          4.36964358e-04, -2.53654551e-04,  9.55251919e-04],\n",
       "        ...,\n",
       "        [-1.17030600e-03, -6.92832400e-04, -7.11159140e-04, ...,\n",
       "         -6.38545083e-04, -4.66100028e-04,  7.38247181e-04],\n",
       "        [-1.38660043e-03, -8.99456616e-04, -9.80616547e-04, ...,\n",
       "         -5.77305560e-04, -2.95191596e-04,  1.14541547e-03],\n",
       "        [-1.50433963e-03, -1.27618515e-03, -1.19612168e-03, ...,\n",
       "         -4.93430707e-04, -1.53671441e-04,  1.60252885e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.11766367e-05, -1.47675528e-04,  1.69841675e-04, ...,\n",
       "          1.88826991e-04,  1.91001673e-05,  2.40910122e-05],\n",
       "        [ 1.04531151e-04, -3.00967513e-04,  2.10043741e-04, ...,\n",
       "          3.00389191e-04, -2.39358094e-04,  2.70517718e-04],\n",
       "        [ 4.09867178e-04, -9.82824713e-05, -3.34681681e-04, ...,\n",
       "          1.80240953e-04, -4.81335766e-04,  2.41174144e-04],\n",
       "        ...,\n",
       "        [ 5.62584843e-04, -7.93037761e-05, -7.49092782e-04, ...,\n",
       "          2.47299293e-04, -1.62271317e-03,  1.28685182e-03],\n",
       "        [ 5.35479339e-04, -5.65063383e-04, -9.02362925e-04, ...,\n",
       "          1.74805245e-04, -1.41939230e-03,  1.60337286e-03],\n",
       "        [ 4.54062450e-04, -1.07512192e-03, -1.07503333e-03, ...,\n",
       "          1.29296808e-04, -1.16964476e-03,  1.97632844e-03]],\n",
       "\n",
       "       [[ 8.11766367e-05, -1.47675528e-04,  1.69841675e-04, ...,\n",
       "          1.88826991e-04,  1.91001673e-05,  2.40910122e-05],\n",
       "        [ 2.29892001e-04, -1.74049695e-04,  3.83280014e-04, ...,\n",
       "          3.05402587e-04, -1.20038630e-05,  9.42980114e-05],\n",
       "        [ 4.31215158e-04,  1.73759472e-04,  7.35177891e-05, ...,\n",
       "          9.87330131e-05, -2.84678274e-04,  1.56561378e-04],\n",
       "        ...,\n",
       "        [ 1.09323196e-03, -4.30903201e-05, -1.11077400e-03, ...,\n",
       "         -1.83789618e-03, -2.51501775e-03,  2.94224243e-04],\n",
       "        [ 8.61952489e-04, -2.62777583e-04, -1.15301099e-03, ...,\n",
       "         -1.93553919e-03, -2.31416337e-03,  5.44881797e-04],\n",
       "        [ 6.72639522e-04, -7.41432072e-04, -1.15397584e-03, ...,\n",
       "         -2.02479842e-03, -2.04857951e-03,  9.29966336e-04]],\n",
       "\n",
       "       [[ 8.11766367e-05, -1.47675528e-04,  1.69841675e-04, ...,\n",
       "          1.88826991e-04,  1.91001673e-05,  2.40910122e-05],\n",
       "        [ 2.31853221e-04, -1.91135638e-04, -7.99798072e-05, ...,\n",
       "          5.63433394e-04, -1.44606165e-05,  7.22864061e-05],\n",
       "        [ 1.85107187e-04, -4.34746180e-04, -3.78310913e-04, ...,\n",
       "          1.03697542e-03,  2.49739853e-04,  1.34801419e-04],\n",
       "        ...,\n",
       "        [-1.30373228e-03, -1.26210344e-03, -7.09353772e-04, ...,\n",
       "          3.90237110e-04,  2.05200427e-04,  5.22416783e-04],\n",
       "        [-1.33157149e-03, -1.77369313e-03, -8.12153856e-04, ...,\n",
       "          3.22695647e-04,  2.77118204e-04,  9.54419433e-04],\n",
       "        [-1.34792866e-03, -2.30092695e-03, -9.36300552e-04, ...,\n",
       "          2.76468636e-04,  3.62828985e-04,  1.44743873e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e63ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.4 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7afa4edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "487/487 [==============================] - 240s 488ms/step - loss: 3.2881 - val_loss: 2.9086\n",
      "Epoch 2/10\n",
      "487/487 [==============================] - 237s 486ms/step - loss: 2.7364 - val_loss: 2.6596\n",
      "Epoch 3/10\n",
      "487/487 [==============================] - 237s 487ms/step - loss: 2.4250 - val_loss: 2.4848\n",
      "Epoch 4/10\n",
      "487/487 [==============================] - 238s 488ms/step - loss: 2.1188 - val_loss: 2.3494\n",
      "Epoch 5/10\n",
      "487/487 [==============================] - 238s 488ms/step - loss: 1.8304 - val_loss: 2.2514\n",
      "Epoch 6/10\n",
      "487/487 [==============================] - 238s 488ms/step - loss: 1.5740 - val_loss: 2.1896\n",
      "Epoch 7/10\n",
      "487/487 [==============================] - 237s 487ms/step - loss: 1.3611 - val_loss: 2.1538\n",
      "Epoch 8/10\n",
      "487/487 [==============================] - 238s 488ms/step - loss: 1.1982 - val_loss: 2.1487\n",
      "Epoch 9/10\n",
      "487/487 [==============================] - 238s 489ms/step - loss: 1.0883 - val_loss: 2.1634\n",
      "Epoch 10/10\n",
      "487/487 [==============================] - 238s 488ms/step - loss: 1.0253 - val_loss: 2.1849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2400114190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=val_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44303fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만든다.\n",
    "    #    1. 입력받은 문장의 텐서를 입력\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑는다.\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙인다.\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마친다.\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7407095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so , <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8d541",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 결론\n",
    "### 3.1 참조\n",
    "- 글자 수 이상문장제외: https://www.kite.com/python/answers/how-to-count-the-number-of-words-in-a-string-in-python\n",
    "- val_loss : https://github.com/JaeHeee/AIFFEL_Project/blob/master/EXPLORATION/EXPLORATION%2011.%20%EC%9E%91%EC%82%AC%EA%B0%80%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 회고\n",
    "- 프로젝트 노드에서 '토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기'라는 내용을 요구하는 문항이 있었다. 이 문항에 대해 어떤 코드를 작성해야 할 지, 또한 요구하는 게 무엇인지 파악이 불가하여 조원 수진님을 통해 참조하셨던 링크를 받아 if 조건문에 continue를 사용하여 코드 작성을 했다. \n",
    "\n",
    "- 프로젝트에서 요구하는 val_loss 값은 2.2였는데 요구하는 값에 딱 맞추기에는 너무나 힘든 과정이었다. embedding_size와 hidden_size를 조절하여 val_loss 값을 낮춰나갔는데, 이 값들의 크기를 높게 조절할 수록 학습 시간은 점점 더 길어졌었다. 중간에 조원중 진환님께서 하이퍼 파라미터를 어떻게 조절하는 게 좋을 지에 대해 조언을 해주셔서 다른 사람들에 비해 비교적 쉽게 val_loss 값을 낮출 수 있었다.\n",
    "\n",
    "- 결론에 대해 짚으면, 마지막에 모델을 통해 생성한 문장의 결과가 찝찝하게 나온 것 같다. 완벽한 문장이 아니라 'i love you so,' 에서 끊겼기 때문에 답답한 심정이다. 원인을 찾지 못했기에 일단은 저 결과에 만족할 수 밖에 없었던 것 같다.\n",
    "\n",
    "- 이번 프로젝트에 대해 요지 자체를 파악하지 못한 것 같아 아쉬움이 많았다. 그만큼 자연어 처리가 어렵고 조금 난해한 분야라는 느낌을 받았다. 아직은 나에게 맞는 분야를 찾아가는 단계이기에 다음 NLP노드에서는 자연어처리의 색 다른 매력이 있을 지 노드를 좀 더 꼼꼼하게 살펴보도록 해야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
