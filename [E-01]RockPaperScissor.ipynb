{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d6fe74",
   "metadata": {},
   "source": [
    "# [E-01]RockPaperScissor (가위바위보 분류기 만들기)\n",
    "## 목차\n",
    "##### 01. 데이터 준비\n",
    "##### 02. 딥러닝 네트워크 설계\n",
    "##### 03. 딥러닝 네트워크 학습\n",
    "##### 04. 테스트(평가)\n",
    "##### 05. 정확도\n",
    "##### 06. 참조\n",
    "##### 07. 회고\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738bae8",
   "metadata": {},
   "source": [
    "## 01. 데이터 준비\n",
    "- Google의 'teachable machine' 사이트를 통해 이미지 데이터 생성\n",
    "- 디렉토리 'rock_scissor_paper' 및 하위 디렉토리 생성, 가위 바위 보 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "cee2144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "823fca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c84d8",
   "metadata": {},
   "source": [
    "- PIL 라이브러리를 설치하는 이유는 '가위,바위,보'이미지의 사이즈를 224x224에서 28x28로 변경하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "bc8d8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31452041",
   "metadata": {},
   "source": [
    "- 파일마다 모두 28x28 사이즈로 바꾸어 target_size에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "c80d8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400  images to be resized.\n",
      "400  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "6441bca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400  images to be resized.\n",
      "400  images resized.\n",
      "바위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "    \n",
    "print(\"바위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "e5c5deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400  images to be resized.\n",
      "400  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049bd14",
   "metadata": {},
   "source": [
    "- resize 메서드를 활용하여 가위, 바위, 보 모두 28x28 사이즈로 resize 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "6c6b708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 1200 입니다.\n",
      "x_train shape: (1200, 28, 28, 3)\n",
      "y_train shape: (1200,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(img_path, number_of_data=1200):  \n",
    "  \n",
    "    img_size=28\n",
    "    color=3\n",
    "\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=1  \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img   \n",
    "        labels[idx]=2  \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   \n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25f252",
   "metadata": {},
   "source": [
    "- load_data() 함수를 통해 이미지가 있는 폴더의 데이터를 부름\n",
    "- 학습데이터의 이미지 개수는 총 1200개\n",
    "- 이미지 데이터와 라벨데이터를 담은 행렬 영역을 생성하여 라벨링 (가위:0, 바위:1, 보:2)\n",
    "- 데이터는 각 픽셀 값의 0~255 범위에 있으므로, 데이터들을 255.0으로 나누어 입력은 0 ~ 1 사이의 값으로 정규화시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "9805c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8aa05",
   "metadata": {},
   "source": [
    "- 데이터셋의 X항목(x_train)은 이미지를 담은 행렬\n",
    "- 데이터셋의 Y항목(y_train)은 실제 숫자를 담은 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92b9a0",
   "metadata": {},
   "source": [
    "## 02. 딥러닝 네트워크 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "fbef5923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))     \n",
    "model.add(keras.layers.Dense(3, activation='softmax'))   \n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9c7a2",
   "metadata": {},
   "source": [
    "- tf.keras의 Sequential API를 이용\n",
    "- Conv2D의 사이즈가 3x3인 필터를 32개, 64개 사용하여 이미지의 특징을 다양화 \n",
    "- 이미지의 색상이 컬러이므로, input_shape=(28,28,3)으로 지정\n",
    "- 복잡한 영상임으로 인지하고 dense의 뉴런 숫자 32를 지정\n",
    "- 최종 분류기의 Class가 가위,바위,보 3종류이므로 3을 기입 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d42d17",
   "metadata": {},
   "source": [
    "- Conv2D는 필터로 특징을 뽑아 주는 Convolution 레이어\n",
    "- MaxPool2D 와 MaxPooling2D 는 이름만 크게 다를 뿐, 컨볼루션 레이어의 출력 이미지에서 주요값만 뽑아 크기가 작은 출력 영상을 만듬\n",
    "- Flatten는 전결합층에 전달하기 위해 1차원 자료로 바꾸어 주는 레이어\n",
    "- Dense 레이어의 첫 번째 인자는 분류기에 사용되는 뉴런 숫자, 마지막 Dense 레이어의 뉴런 숫자는 결과적으로 분류해내야하는 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "b7360528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_90 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_91 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 70,723\n",
      "Trainable params: 70,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7a2c6",
   "metadata": {},
   "source": [
    "## 03. 딥러닝 네트워크 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "718a7456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 1.0062 - accuracy: 0.5083\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6483 - accuracy: 0.7717\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.9508\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9817\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0541 - accuracy: 0.9967\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9967\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 9.3191e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 8.2364e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 7.4566e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96207c0580>"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6263d",
   "metadata": {},
   "source": [
    "- 'epochs = 20' : 전체 데이터를 20번 반복 사용하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc67b5",
   "metadata": {},
   "source": [
    "## 04. 테스트(평가)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "89c7f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위_test 이미지 resize 완료!\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "바위_test 이미지 resize 완료!\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "보_test 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor_test\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위_test 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock_test\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위_test 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper_test\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보_test 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad63ed6",
   "metadata": {},
   "source": [
    "- 위와 동일하게 resize 메서드를 활용하여 가위_test, 바위_test, 보_test 모두 28x28 사이즈로 resize 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "bf966549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "def load_data_test(img_path, number_of_data=300):  \n",
    "  \n",
    "    img_size=28\n",
    "    color=3\n",
    "\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor_test/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock_test/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=1  \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_test/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img   \n",
    "        labels[idx]=2  \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data_test(image_dir_path)\n",
    "x_test_norm = x_test/255.0   \n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "0d8af60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1.6679 - accuracy: 0.7233\n",
      "test_loss: 1.6679376363754272 \n",
      "test_accuracy: 0.7233333587646484\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe423cfe",
   "metadata": {},
   "source": [
    "## 5. 정확도\n",
    "- 학습 데이터 300장, 테스트 데이터 300장, Epoch==10, accuracy: 0.1200\n",
    "- 학습 데이터 600장, 테스트 데이터 300장, Epoch==10, accuracy: 0.3333\n",
    "- 학습 데이터 600장, 테스트 데이터 300장, Epoch==30, accuracy: 0.4067\n",
    "- 학습 데이터 1200장, 테스트 데이터 300장, Epoch==10, accuracy: 0.3067\n",
    "- 학습 데이터 1200장, 테스트 데이터 300장, Epoch==30, accuracy: 0.3100\n",
    "- 학습 데이터 2400장, 테스트 데이터 300장, Epoch==10, accuracy: 0.3899\n",
    "- 학습 데이터 2400장, 테스트 데이터 300장, Epoch==30, accuracy: 0.4133 \n",
    "- 학습 데이터 2400장, 테스트 데이터 300장, Epoch==30, accuracy: 0.4800 (Conv2D 필터의 개수 조정 각각 32,64 -> 64,128)\n",
    "- 학습 데이터 2400장, 테스트 데이터 300장, Epoch==30, accuracy: 0.5867 (dense: 32 -> 64)\n",
    "> 목표한 정확도(60%)에 가까워졌기에 테스트 데이터의 이미지를 수정할 필요를 느낌.  테스트 데이터의 사진을 다른 데이터로 변경해보고, 개수도 조정해봄\n",
    "\n",
    "- 학습 데이터 2400장, 테스트 데이터 240장, Epoch==30, accuracy: 0.3833 \n",
    "- 학습 데이터 2400장, 테스트 데이터 240장, Epoch==30, accuracy: 0.3958 (Conv2D 필터의 개수 다시 조정 각각 64,128 ->128,256)\n",
    "> 기존의 학습 데이터에서 질이 좋지 않은 이미지를 300장 제거해봄\n",
    " \n",
    "- 학습 데이터 2100장, 테스트 데이터 240장, Epoch==30, accuracy: 0.4000\n",
    "> 여기서 학습 데이터량만 높이면 된다고 판단함\n",
    "\n",
    "- 학습 데이터 3300장, 테스트 데이터 300장, Epoch==30, accuracy: 0.2000\n",
    "> 무작정 학습 데이터량만 늘렸을 뿐인데 정확도는 오히려 낮아졌기에 학습데이터의 질을 높여 좋은 데이터만으로 학습을 다시 시작했음\n",
    "\n",
    "Conv 24D 필터 각각: 32,64 / dense:32로 model 값은 건들지 않고 학습량만 생각을 해보겠다는 판단을 함\n",
    "- 학습 데이터 600장, 테스트 데이터 300장, Epoch==20, accuracy: 0.5067\n",
    "- 학습 데이터 1200장, 테스트 데이터 300장, Epoch==20, accuracy: 0.7233\n",
    "> 목표하던 정확도 이상의 결과를 보였음.\n",
    "\n",
    "## 정확도 = 72%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6c79b",
   "metadata": {},
   "source": [
    "## 6. 참조\n",
    "#### 각 레이어의 명칭과 쓰임을 참조하였음 : https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4864c",
   "metadata": {},
   "source": [
    "## 7. 회고\n",
    "#### - 처음 진행하던 프로젝트 이기도 했고, 아직까지 파이썬과 각종 용어에 대한 지식이 미숙한 편이었기에 코드 작성에 조금 까다로움을 느꼈던 것 같다. 코드에 대한 내용의 큰 틀은 파악했으나, 아직 명칭과 쓰임에 대해서 미숙하여 전반적인 내용 학습이 더 필요하다 느꼈다.\n",
    "\n",
    "#### - 일단 결과물에 대해서는 스스로 만족하는 것 같다. 처음에 학습 데이터 300개와 테스트 데이터 300개의(비율 1:1) 이미지를 가지고 정확도 판단을 시작했다. 그러면서 학습데이터만 점차 2배씩 늘려갔고 중간에 하이퍼파라미터를 과도하게 조절하기도 했다. 원하는 정확도에 거의 도달했을 쯤에 테스트데이터의 이미지 질이 좋지 않다고 판단하여 새롭게 변경하고 개수도 조정했다. 그러고나서 학습 데이터를 늘리니 오히려 오버피팅의 결과를 일으켰다. 원인을 찾지 못해 하이퍼파라미터를 계속 조절해도 정확도는 쉽게 오르지 않았다. 그래서 내린 생각은 적절한 하이퍼파라미터 값을 정해놓고, 질이 좋은 학습데이터만을 구성하여 다시 학습량을 점차 늘려가자는 것이 결론이었다. 그랬더니 결국 목표하던 정확도 이상의 결과값을 얻어낼 수 있었다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13b0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
